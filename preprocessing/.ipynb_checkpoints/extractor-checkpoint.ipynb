{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65151b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity='all'\n",
    "from urllib.parse import urlparse,urlsplit\n",
    "from IPy import IP\n",
    "import re\n",
    "from tld import get_tld,get_fld,parse_tld,is_tld\n",
    "import statistics\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import nmap\n",
    "import subprocess\n",
    "import logging \n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "# import bitly_api\n",
    "# import BitlyAPI\n",
    "# from BitlyAPI import shorten_urls\n",
    "import json\n",
    "from collections import Counter\n",
    "import whois\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import http.client, urllib\n",
    "from xpinyin import Pinyin\n",
    "pd.set_option('display.width',1000)\n",
    "pd.set_option('display.max_columns',1000)\n",
    "pd.set_option('display.max_colwidth',1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effcd896",
   "metadata": {},
   "source": [
    "# 53 features from spacephish, refer to table III in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7ebd840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMalicious scripts can be added to legitimate pages. Some file extensions used in\\nURL paths may lunch such kind of attacks. Presence of the following malicious\\npath extensions is considered: ’txt’, ’exe’, ’js’\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\ncheck how many sensitive word in the url\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\nIn well-formed URLs, top-level domains (TLDs) appear only before the path.\\nWhen TLDs appear in the path (f30) or in the subdomain part (f31), the URL is\\nconsidered phishing.\\nfld:first-level domain\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\nIn well-formed URLs, top-level domains (TLDs) appear only before the path.\\nWhen TLDs in the subdomain part (f31), the URL is considered phishing.\\nfld:first-level domain\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\nNatural language processing and word-raw features are also used in phishing\\ndetection. We consider number of words (f40), char repeat (f41), shortest\\nwords in URLs (f42), hostnames (f43), and paths (f44), longest words in URLs\\n(f45), hostnames (f46), and paths (f47), average length of words in URLs (f48),\\nhostnames (f49), and paths (f50).\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URL features\n",
    "\n",
    "def checkLength(URL):#2\n",
    "    '''\n",
    "    Phishers can use long URL to hide the doubtful part in the address bar.\n",
    "    if len(url)>75->phishing,<54->legit，else:0 suspicious\n",
    "    '''\n",
    "    legitimate_threshold = 54\n",
    "    suspicious_threshold = 75\n",
    "    #return(len(URL))\n",
    "    if(len(URL)) < legitimate_threshold:\n",
    "        return -1\n",
    "    elif(len(URL)) < suspicious_threshold:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "def hexDecoder(domain):\n",
    "    '''\n",
    "    Function that inspects a given domain to check if it is a hex-encoded IP address.\n",
    "    If the domain is an hex-encoded IP address, it return the IPv4 address;\n",
    "    if the domain is not an IP address, it returns 0\n",
    "    '''\n",
    "    try:\n",
    "        n = domain.split(\".\")\n",
    "        IPv4 = str(int(n[0],16))\n",
    "        for number in n[1:]:\n",
    "            IPv4 = IPv4 + \".\" + str(int(number,16))\n",
    "        return IPv4\n",
    "    except:\n",
    "        return 0\n",
    "def checkIP(URL):#1\n",
    "    '''\n",
    "    Function that inspects a given URL to determine if it contains an IP address.\n",
    "    If returns 1 if it is an IP address, and -1 otherwise\n",
    "    ''' \n",
    "    if(URL.count('.')==1 and URL.startswith('http')==False):\n",
    "        domain = URL\n",
    "    else:\n",
    "        domain = ((urlparse(URL)).netloc)\n",
    "    \n",
    "    try:\n",
    "        i = IP(domain)\n",
    "        #print(\"{} is a valid IP address\".format(domain))\n",
    "        i = 1\n",
    "    except:\n",
    "        decoded = hexDecoder(domain)\n",
    "        if (decoded==0):\n",
    "            i = -1\n",
    "            #print(\"{} is not a valid IP address\".format(domain))\n",
    "            return i\n",
    "        try:\n",
    "            i = IP(decoded)\n",
    "            #print(\"{} is an IP address in hexadecimal format\".format(domain))\n",
    "            i = 1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            i = -1\n",
    "    return i\n",
    "\n",
    "def checkRedirect(URL):\n",
    "    '''\n",
    "    The existence of “//” within the URL path means that the user will be redirected to another website. \n",
    "    An example of such URL’s is: “http://www.legitimate.com//http://www.phishing.com”.phishing:1,legit:-1\n",
    "    '''\n",
    "    if (URL.rfind(\"//\") > 7):\n",
    "        redirect = 1\n",
    "    else:\n",
    "        redirect = -1\n",
    "    return redirect\n",
    "def checkShortener(URL):#3 how to get the list?change the list\n",
    "    '''\n",
    "    tidy url. \n",
    "    '''\n",
    "    shorteners_list = [\"bit.do\",\"t.co\",\"lnkd.in\",\"db.tt\",\"qr.ae\",\"adf.ly\",\"goo.gl\",\n",
    "                       \"bitly.com\",\"cur.lv\",\"tinyurl.com\",\"ow.ly\",\"bit.ly\",\"ity.im\",\n",
    "                       \"q.gs\",\"is.gd\",\"po.st\",\"bc.vc\",\"twitthis.com\",\"u.to\",\"j.mp\",\n",
    "                       \"buzurl.com\",\"cutt.us\",\"u.bb\",\"yourls.org\",\"x.co\",\"prettylinkpro.com\",\n",
    "                       \"scrnch.me\",\"filoops.info\",\"vzturl.com\",\"qr.net\",\"1url.com\",\"tweez.me\",\n",
    "                       \"v.gd\",\"tr.im\",\"link.zip.net\",\"tinyarrows.com\",\"adcraft.co\",\"adcrun.ch\",\n",
    "                       \"adflav.com\",\"aka.gr\",\"bee4.biz\",\"cektkp.com\",\"dft.ba\",\"fun.ly\",\"fzy.co\",\n",
    "                       \"gog.li\",\"golinks.co\",\"hit.my\",\"id.tl\",\"linkto.im\",\"lnk.co\",\"nov.io\",\"p6l.org\",\n",
    "                       \"picz.us\",\"shortquik.com\",\"su.pr\",\"sk.gy\",\"tota2.com\",\"xlinkz.info\",\"xtu.me\",\n",
    "                       \"yu2.it\",\"zpag.es\"]\n",
    "    for s in shorteners_list:\n",
    "        if(URL.find(s+\"/\")>-1):#if didn't find return -1\n",
    "            return 1\n",
    "    return -1\n",
    "#check the number of dots in subdomain,if the dots are greater than two, it is classified as “Phishing”\n",
    "def checkSubdomains(URL):\n",
    "    '''\n",
    "    If the number of dots (aside from the \"WWW\" and the \"ccTLD\") is greater than one, \n",
    "    then the URL is classified as “Suspicious” since it has one sub domain. \n",
    "    However, if the dots are greater than two, it is classified as “Phishing” since it will have multiple sub domains.\n",
    "    else if the number of dots is one,no sub domain->legit\n",
    "    '''\n",
    "    if(URL.count('.')==1 and URL.startswith('http')==False):\n",
    "        ind=URL.find('/')\n",
    "        if ind>-1:\n",
    "            domain=URL[:ind]\n",
    "        else:\n",
    "            domain = URL\n",
    "    else:\n",
    "        domain = ((urlparse(URL)).netloc).lower()\n",
    "        \n",
    "    if (domain.startswith(\"www.\")):\n",
    "        domain = domain[4:]\n",
    "    counter = domain.count('.') - 2 #we subtract 2 to account for possible ccTLDs\n",
    "    if(counter) > 0:\n",
    "        return 1\n",
    "    elif(counter)==0:\n",
    "        return 0#suspicious\n",
    "    else:\n",
    "        return -1#legit\n",
    "def checkAt(URL):\n",
    "    '''\n",
    "    Using “@” symbol in the URL leads the browser to ignore everything preceding the “@” symbol \n",
    "    and the real address often follows the “@” symbol. \n",
    "    '''\n",
    "    if (URL.find(\"@\") >= 0):\n",
    "        at = 1\n",
    "    else:\n",
    "        at = -1\n",
    "    return at\n",
    "def checkDash(URL): #special list\n",
    "    '''\n",
    "    The dash symbol is rarely used in legitimate URLs. \n",
    "    For example http://www.Confirme-paypal.com/.\n",
    "    '''\n",
    "    if(URL.count('.')==1 and URL.startswith('http')==False):\n",
    "        ind=URL.find('/')\n",
    "        if ind>-1:\n",
    "            domain=URL[:ind]\n",
    "        else:\n",
    "            domain = URL\n",
    "    else:\n",
    "        domain = ((urlparse(URL)).netloc).lower()\n",
    "        \n",
    "    if(domain.find(\"-\")>-1):\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "#check number of common terms\n",
    "def checkNumberofCommonTerms(URL):\n",
    "    '''\n",
    "    the common terms \"http,//,.com,www\" only appear once in legit.\n",
    "    '''\n",
    "    url=URL.lower()\n",
    "    #print(url)\n",
    "    common_term=['http','www','.com','//']\n",
    "    for term in common_term:\n",
    "        if(url.count(term)>1):\n",
    "            #print(term,url.count(term))\n",
    "            return 1\n",
    "        else:\n",
    "            #print(term,url.count(term))\n",
    "            continue\n",
    "    return -1\n",
    "#check numerical,Numerical characters are uncommon for benign domains and especially subdomains in\n",
    "#our dataset. check if the domain and subdomain include number,if has number->phishing\n",
    "def checkNumerical(URL):\n",
    "    try:\n",
    "        res=get_tld(URL,as_object=True) \n",
    "    except:\n",
    "        return 1\n",
    "    domain=res.subdomain+res.domain\n",
    "    number = re.search(r'\\d+', domain)\n",
    "    if number:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "#path extension, if txt exe and js in the path->phishing, legit otherwise\n",
    "'''\n",
    "Malicious scripts can be added to legitimate pages. Some file extensions used in\n",
    "URL paths may lunch such kind of attacks. Presence of the following malicious\n",
    "path extensions is considered: ’txt’, ’exe’, ’js’\n",
    "\n",
    "'''\n",
    "def checkPathExtend(URL):\n",
    "    extension=['.txt','.exe','.js']\n",
    "    if(URL.count('.')==1 and URL.startswith('http')==False):\n",
    "        ind=URL.find('/')\n",
    "        if ind>-1:\n",
    "            path = URL[ind:]\n",
    "        else:\n",
    "            path=None\n",
    "        \n",
    "    else:\n",
    "        path=(urlparse(URL).path).lower()\n",
    "    #print('path is:',path)\n",
    "    if path:\n",
    "        for ex in extension:\n",
    "            #print('find',ex,path.find(ex))\n",
    "            if path.find(ex)>-1:\n",
    "                return 1 \n",
    "            \n",
    "    return -1\n",
    "#Punycode is used in domain names to replace some ASCIIs with Unicode，check if include unicode->phishing\n",
    "#characters. URLs will then look legitimate where they refer to different websites.\n",
    "#URLs with punycodes are considered phishing.\n",
    "#check the domain if start with xn-- or end with - ->phishing, legit otherwise\n",
    " \n",
    "\n",
    "def checkPunycode(URL): \n",
    "    if(URL.count('.')==1 and URL.startswith('http')==False):\n",
    "        ind=URL.find('/')\n",
    "        if ind>-1:\n",
    "            domain = URL[:ind]\n",
    "        else:\n",
    "            domain=URL\n",
    "        \n",
    "    else:\n",
    "        domain = ((urlparse(URL)).netloc).lower()\n",
    "    #print('domain is',domain)\n",
    "    subdomain=domain.split('.')\n",
    "    #print('subdomain is',subdomain)\n",
    "    for i in subdomain:\n",
    "        mat=re.search('^xn--[a-z0-9]{1,59}|-$',i)\n",
    "        if mat:\n",
    "            return 1\n",
    "\n",
    "    return -1 \n",
    " \n",
    "'''\n",
    "check how many sensitive word in the url\n",
    "'''\n",
    "def checkSensitiveWord(URL): \n",
    "    sensitive_words=['secure','account','webscr','login','ebayisapi','signin','banking','confirm']\n",
    "    counts=0\n",
    "    for word in sensitive_words:\n",
    "        num=URL.count(word)\n",
    "        #print(num)\n",
    "        counts=counts+num\n",
    "    return counts\n",
    "       \n",
    "#check the TLD position in path, if in path->phishing,1,else legit->-1\n",
    "#get read file to get TLD_LIST inadvance\n",
    "'''\n",
    "In well-formed URLs, top-level domains (TLDs) appear only before the path.\n",
    "When TLDs appear in the path (f30) or in the subdomain part (f31), the URL is\n",
    "considered phishing.\n",
    "fld:first-level domain\n",
    "'''\n",
    "\n",
    "def checkTLDinPath(URL):\n",
    "    try:\n",
    "        res=get_tld(URL, as_object=True,fix_protocol=True)\n",
    "    except:\n",
    "        return -1\n",
    "    path=res.parsed_url.path\n",
    "    #print('path',path)\n",
    "    if path:\n",
    "        path=path.lower().split('.')\n",
    "        for pa in path:\n",
    "            if is_tld(pa)==True:\n",
    "                return 1\n",
    "    return -1\n",
    "#check the TLD position in path, if in path->phishing,1,else legit->-1\n",
    "#get read file to get TLD_LIST inadvance\n",
    "'''\n",
    "In well-formed URLs, top-level domains (TLDs) appear only before the path.\n",
    "When TLDs in the subdomain part (f31), the URL is considered phishing.\n",
    "fld:first-level domain\n",
    "'''\n",
    "\n",
    "def checkTLDinSub(URL):\n",
    "    try:\n",
    "        res=get_tld(URL, as_object=True,fix_protocol=True)\n",
    "    except:\n",
    "        return -1\n",
    "    sub_domain=res.subdomain\n",
    "    #print('subdomain',sub_domain)\n",
    "    if sub_domain:\n",
    "        sub=sub_domain.lower().split('.')\n",
    "        for s in sub:\n",
    "            if is_tld(s)==True:\n",
    "                return 1\n",
    "    return -1\n",
    "#NLP feature, get the number of words \n",
    "def totalWordUrl(URL):\n",
    "    res = re.split(r\"[/:\\.?=\\&\\-\\s\\_]+\",URL)\n",
    "    #print('res',res)\n",
    "    total=len(res) \n",
    "    return total\n",
    "#NLP features\n",
    "'''\n",
    "Natural language processing and word-raw features are also used in phishing\n",
    "detection. We consider number of words (f40), char repeat (f41), shortest\n",
    "words in URLs (f42), hostnames (f43), and paths (f44), longest words in URLs\n",
    "(f45), hostnames (f46), and paths (f47), average length of words in URLs (f48),\n",
    "hostnames (f49), and paths (f50).\n",
    "'''\n",
    " \n",
    "def shortestWordUrl(URL):\n",
    "    res = re.split(r\"[/:\\.?=\\&\\-\\s\\_]+\",URL)\n",
    "    #print('res',res)\n",
    "    try:\n",
    "        shortest=min((word for word in res if word), key=len)\n",
    "        return len(shortest)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def shortestWordHost(URL):\n",
    "    hostname=urlparse(URL).netloc\n",
    "    res=hostname.split('.')\n",
    "    try:\n",
    "        shortest=min((word for word in res if word), key=len)\n",
    "        return len(shortest)\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "#shortest word in path\n",
    " \n",
    "def shortestWordPath(URL): \n",
    "    if(URL.startswith('http')==False):\n",
    "        ind=URL.find('/')\n",
    "        if ind>-1:\n",
    "            path = URL[ind:] \n",
    "        else:\n",
    "            path=None\n",
    "    else: \n",
    "        path=(urlparse(URL).path).lower()\n",
    "    #print('path',path)\n",
    "    res = re.split(r\"[/:\\.?=\\&\\-\\s\\_]+\",path)\n",
    "    #print('res',res)\n",
    "    try:\n",
    "        shortest=min((word for word in res if word), key=len)\n",
    "        return len(shortest)\n",
    "    except:\n",
    "        return 0\n",
    "        \n",
    "    \n",
    "#longest word in URL\n",
    " \n",
    "def longestWordUrl(URL):\n",
    "    res = re.split(r\"[/:\\.?=\\&\\-\\s\\_]+\",URL)\n",
    "    #print('res',res)\n",
    "    try:\n",
    "        longest=max((word for word in res if word), key=len)\n",
    "    except:\n",
    "        return 0\n",
    "    return len(longest)\n",
    "\n",
    "def longestWordHost(URL):\n",
    "    if(URL.startswith('http')==False):\n",
    "        ind=URL.find('/')\n",
    "        if ind>-1:\n",
    "            hostname = URL[:ind] \n",
    "        else:\n",
    "            hostname=URL\n",
    "    else: \n",
    "        hostname= urlparse(URL).hostname\n",
    "    #print('hostname',hostname)\n",
    "    res = re.split(r\"[/:\\.?=\\&\\-\\s\\_]+\",hostname)\n",
    "    #print('res',res)\n",
    "    try:\n",
    "        longest=max((word for word in res if word), key=len)\n",
    "    except:\n",
    "        return 0\n",
    "    return len(longest)\n",
    "def longestWordPath(URL): \n",
    "    if(URL.startswith('http')==False):\n",
    "        ind=URL.find('/')\n",
    "        if ind>-1:\n",
    "            path = URL[ind:] \n",
    "        else:\n",
    "            path=None\n",
    "    else: \n",
    "        path=(urlparse(URL).path).lower()\n",
    "    #print('path',path)\n",
    "    res = re.split(r\"[/:\\.?=\\&\\-\\s\\_]+\",path)\n",
    "    #print('res',res)\n",
    "    try:\n",
    "        longest=max((word for word in res if word), key=len)\n",
    "        return len(longest)\n",
    "    except:\n",
    "        return 0\n",
    "def averageWordUrl(URL):\n",
    "    res = re.split(r\"[/:\\.?=\\&\\-\\s\\_]+\",URL)\n",
    "    #print('res',res)\n",
    "    average=statistics.mean((len(word) for word in res if word))\n",
    "    return format(average,'.2f')\n",
    "def averageWordHost(URL):\n",
    "    if(URL.startswith('http')==False):\n",
    "        ind=URL.find('/')\n",
    "        if ind>-1:\n",
    "            hostname = URL[:ind] \n",
    "        else:\n",
    "            hostname=URL\n",
    "    else: \n",
    "        hostname= urlparse(URL).hostname\n",
    "    #print('hostname',hostname)\n",
    "    res = re.split(r\"[/:\\.?=\\&\\-\\s\\_]+\",hostname)\n",
    "    #print('res',res)\n",
    "    average=statistics.mean((len(word) for word in res if word))\n",
    "    return format(average,'.2f')\n",
    "def averageWordPath(URL): \n",
    "    if(URL.startswith('http')==False):\n",
    "        ind=URL.find('/')\n",
    "        if ind>-1:\n",
    "            path = URL[ind:] \n",
    "        else:\n",
    "            path=None\n",
    "    else: \n",
    "        path=(urlparse(URL).path).lower()\n",
    "    #print('path',path)\n",
    "    res = re.split(r\"[/:\\.?=\\&\\-\\s\\_]+\",path)\n",
    "    #print('res',res)\n",
    "    try:\n",
    "        average=statistics.mean((len(word) for word in res if word))\n",
    "        return format(average,'.2f')\n",
    "    except:\n",
    "        return 0\n",
    "#collect top 10 domain from phishtank, if url's firstdomain include these domain,means->phishing,else legit,if tld in top_tld->suspicious\n",
    "def checkStatisticRe(URL):\n",
    "        top_fdomains=['esy.es','hol.es','000webhostapp.com','for-our.info','bit.ly','16mb.com','96.lt','totalsolution.com.br','beget.tech','sellercancelordernotification.com']\n",
    "        top_tld=['surf','cn','bid','gq','ml','cf','work','cam','ga','casa','tk','ga','top','cyou','bar','rest']\n",
    "        \n",
    "        #print('f_domain',f_domain)\n",
    "        try:\n",
    "            f_domain=get_fld(URL)\n",
    "            t_domain=get_tld(URL)\n",
    "        except:\n",
    "            return 1\n",
    "        #print('t_domain',t_domain)\n",
    "        for f in top_fdomains:\n",
    "            if f_domain.find(f)>-1:\n",
    "                return 1\n",
    "        else:\n",
    "            for t in top_tld:\n",
    "                if t_domain.find(t)>-1:\n",
    "                    return 0\n",
    "        return -1 \n",
    "#27 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13c60d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncheck the null value in the footer\\nMost of the legitimate websites do not often use Null links in the footer section of a website, but the phishing sites\\nusually insert null links in the footer section to make the user stay on the same page until the sensitive\\ninformation is submitted by the user,calculate suspicious_anchor/anchors\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#HTML features\n",
    "def getObjects(HTML):\n",
    "    soup = BeautifulSoup(HTML, \"html.parser\")\n",
    "    images = soup.findAll(\"img\")\n",
    "    links = soup.findAll(\"link\")\n",
    "    anchors = soup.findAll(\"a\")\n",
    "    sounds = soup.findAll(\"sound\")\n",
    "    videos = soup.findAll(\"video\")\n",
    "    objects = images + links + anchors + sounds + videos\n",
    "    return objects\n",
    "\n",
    "def sameAuthors(element_location, URL):\n",
    "    '''\n",
    "    Function to determine if two URLs are made by the same authors.\n",
    "    If the first URL contains one \"important\" word within the second URL, then it returns True.\n",
    "    Otherwise it returns False\n",
    "    '''\n",
    "    \n",
    "    element_domain=((urlparse(element_location)).netloc).lower()\n",
    "    if len(element_domain)==0:\n",
    "        return False\n",
    "    if(URL.count('.')==1 and URL.startswith('http')==False):\n",
    "        ind=URL.find('/')\n",
    "        if ind>-1:\n",
    "            domain=URL[:ind]\n",
    "        else:\n",
    "            domain = URL \n",
    "    else:\n",
    "        domain = ((urlparse(URL)).netloc).lower()\n",
    "    domain_words = domain.split(\".\")\n",
    "    words_to_check = []\n",
    "    for word in domain_words:\n",
    "        if len(word) > 3:\n",
    "            words_to_check.append(word)\n",
    "    for word in words_to_check:\n",
    "        if (element_domain.find(word) > -1):\n",
    "            return True\n",
    "    return False\n",
    "def isInternal(element_location, URL):\n",
    "    '''\n",
    "    Function that determines if the location of an HTML element within a webpage is from a different website (External) or not (Internal).\n",
    "    An element is \"Internal\" if its URI adopts relative paths or if its sourced from a webpage from the same (likely) authors as the current one.\n",
    "    This function returns False if the element is sourced from an external site, and True otherwise.\n",
    "    '''\n",
    "    if(element_location.startswith(\"http\")):\n",
    "        return (sameAuthors(element_location, URL))\n",
    "    return True\n",
    "#========================\n",
    "    \n",
    "def checkObjects(objects,HTML, URL):\n",
    "    '''\n",
    "    Function that checks how many objects embedded in the webpage are from external websites.\n",
    "    The return value depends on the rate of suspicious anchors, which is compared against 2 thresholds (suspicious (0) and phising (1)).\n",
    "    '''\n",
    "    suspicious_threshold = 0.15 #0.22\n",
    "    phishing_threshold = 0.3#0.61\n",
    "    soup = BeautifulSoup(HTML, \"html.parser\")\n",
    "    if(len(objects)==0):\n",
    "        return -1 #no embedded objects in html\n",
    "    external_objects = []\n",
    "    object_locations = []\n",
    "    for o in objects:\n",
    "        try:\n",
    "            object_location = o['src']\n",
    "            object_locations.append(object_location)\n",
    "        except:\n",
    "            try:\n",
    "                object_location = o['href']\n",
    "                object_locations.append(object_location)\n",
    "            except:\n",
    "                continue\n",
    "        if(not(isInternal(object_location, URL))):\n",
    "            external_objects.append(o)\n",
    "    if(len(object_locations)==0):\n",
    "        #print(\"no linked objects in html of url: {}\".format(URL))\n",
    "        return 0 #no linked objects in html\n",
    "    external_objects_rate = len(external_objects)/len(object_locations)\n",
    "    '''\n",
    "    print('external_objects are',external_objects)\n",
    "    print('objection lation are',object_locations)\n",
    "    print('length of external_objects is',len(external_objects))\n",
    "    print('length of objection_location is',len(object_locations))\n",
    "    print('external_objects_rate',external_objects_rate)\n",
    "    '''\n",
    "    if(external_objects_rate < suspicious_threshold):\n",
    "        return -1\n",
    "    elif(external_objects_rate < phishing_threshold):\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "#check the meta links only in the meta tags, not include the internal links, \n",
    "#means: only check<script src='http:...',not check<script><a href='http://...'</script>\n",
    "def checkMetaScripts(HTML, URL):\n",
    "    '''\n",
    "    Function that checks the percentage of scripts and metas that share the same domain as the page URL.\n",
    "    The return value depends on the percentage of external, which is compared against 2 thresholds (suspicious (0) and phishing (1)).\n",
    "    '''\n",
    "    suspicious_threshold = 0.52\n",
    "    phishing_threshold = 0.61\n",
    "    soup = BeautifulSoup(HTML, \"html.parser\")\n",
    "    metas = soup.findAll(\"meta\")\n",
    "    scripts = soup.findAll(\"script\")\n",
    "    links=soup.findAll(\"link\")\n",
    "    objects = metas + scripts+links\n",
    "    if(len(objects)==0):\n",
    "        return -1 #no embedded objects in html\n",
    "    external_objects = []\n",
    "    object_locations = []\n",
    "    for o in objects:\n",
    "        object_location = \"\"\n",
    "        keys = o.attrs.keys()\n",
    "        if \"src\" in keys:\n",
    "            object_location = o['src']\n",
    "            object_locations.append(object_location)\n",
    "        elif \"href\" in keys:\n",
    "            object_location = o['href']\n",
    "            object_locations.append(object_location)\n",
    "        elif \"http-equiv\" in keys:\n",
    "            if \"content\" in keys:\n",
    "                content = o.attrs['content']\n",
    "                content_split = content.split(\"URL=\")\n",
    "                if len(content_split)>1:\n",
    "                    object_location = content_split[1].strip()\n",
    "                    object_locations.append(object_location)\n",
    "        if object_location == \"\":\n",
    "            continue\n",
    "        if(not(isInternal(object_location, URL))):\n",
    "            external_objects.append(o)\n",
    "    if(len(object_locations)==0):\n",
    "        #print(\"no linked meta_scripts in html of url: {}\".format(URL))\n",
    "        return -1 #no linked objects in html\n",
    "    external_objects_rate = len(external_objects)/len(object_locations)\n",
    "    #return external_objects_rate\n",
    "\n",
    "    if(external_objects_rate < suspicious_threshold):\n",
    "        return -1\n",
    "    elif(external_objects_rate < phishing_threshold):\n",
    "        return 0\n",
    "    return 1\n",
    "def checkFrequentDomain(objects,HTML,URL):#HTML,URL\n",
    "    '''\n",
    "    This feature examines all the anchor links in source code of a website and compares the most frequent domain\n",
    "    with the local domain of a website.If both the domains are similar then the feature is set to 0.\n",
    "    get the most frequency of ex_domain, if it>the frequency of in_domain, means the most frequent domain is ex_domain 1phishing\n",
    "    '''\n",
    "    soup = BeautifulSoup(HTML, \"html.parser\")\n",
    "    #get the frequency of external domain \n",
    "    if(len(objects)==0):\n",
    "        #print('no objects')\n",
    "        return -1 #no embedded objects in html\n",
    "    object_locations = []\n",
    "    external_locations=[]\n",
    "    internal_objects=[]\n",
    "    ex_domains=[]\n",
    "    frequency_in=0\n",
    "    for o in objects:\n",
    "        try:\n",
    "            object_location = o['src']\n",
    "            object_locations.append(object_location)\n",
    "        except:\n",
    "            try:\n",
    "                object_location = o['href']\n",
    "                object_locations.append(object_location)\n",
    "            except:\n",
    "                continue \n",
    "        if isInternal(object_location, URL):\n",
    "            frequency_in=frequency_in+1\n",
    "        else:\n",
    "            ex_domain =((urlparse(object_location)).netloc).lower()\n",
    "            ex_domains.append(ex_domain)\n",
    "    #print('object_locations',object_locations) \n",
    "    ex_domains=[ x for x in ex_domains if \"w3.org\" not in x ]\n",
    "    if(len(ex_domains)==0):\n",
    "        #print('ex_domain is none')\n",
    "        return -1\n",
    "    #print('ex_domains',ex_domains)\n",
    "    try:\n",
    "        frequent_ex=max(set(ex_domains), key = ex_domains.count)\n",
    "    except:\n",
    "        return -1\n",
    "    #print('frequent_ex',frequent_ex)\n",
    "    try:\n",
    "        frequency_ex=max(ex_domains.count(b) for b in ex_domains if b)\n",
    "    except:\n",
    "        frequency_ex=0\n",
    "            \n",
    "    #print('frequency_ex',frequency_ex)  \n",
    "    #print('frequency_in is',frequency_in)\n",
    "    \n",
    "   #compare frequent of internal or external\n",
    "    if frequency_in>=frequency_ex:\n",
    "        #print('===========')\n",
    "        return -1\n",
    "    else:\n",
    "        #print('***************')\n",
    "        return 1\n",
    "def checkCommonPageRatioinWeb(objects,HTML,URL):\n",
    "    soup = BeautifulSoup(HTML, \"html.parser\")\n",
    "    #get the frequency of external domain    \n",
    "    metas = soup.findAll(\"meta\")\n",
    "    scripts = soup.findAll(\"script\")\n",
    "    objects = objects+metas + scripts\n",
    "    if(len(objects)==0):\n",
    "        #print('no objects')\n",
    "        return 0 #no embedded objects in html\n",
    "    object_locations = []\n",
    "    external_locations=[]\n",
    "    internal_objects=[]\n",
    "    ex_domains=[]\n",
    "    frequency_in=0\n",
    "    for o in objects:\n",
    "        try:\n",
    "            object_location = o['src']\n",
    "            object_locations.append(object_location)\n",
    "        except:\n",
    "            try:\n",
    "                object_location = o['href']\n",
    "                object_locations.append(object_location)\n",
    "            except:\n",
    "                continue \n",
    "        if isInternal(object_location, URL):\n",
    "            frequency_in=frequency_in+1\n",
    "        else:\n",
    "            ex_domain =((urlparse(object_location)).netloc).lower()\n",
    "            ex_domains.append(ex_domain)\n",
    "    #print('object_locations',object_locations) \n",
    "    if(len(object_locations)==0):\n",
    "        #print('no objects')\n",
    "        return 0 #no embedded url in html\n",
    "    #print('ex_domains',ex_domains)\n",
    "    if len(ex_domains)>0:\n",
    "        try:\n",
    "            frequency_ex=max(ex_domains.count(b) for b in ex_domains if b)\n",
    "        except:\n",
    "            frequency_ex=0\n",
    "            \n",
    "    else:\n",
    "        frequency_ex=0\n",
    "    \n",
    "    #print('frequency_ex',frequency_ex)  \n",
    "    #print('frequency_in is',frequency_in)\n",
    "    if frequency_in>=frequency_ex:\n",
    "        most_frequent=frequency_in\n",
    "    else:\n",
    "        most_frequent=frequency_ex\n",
    "    total=len(object_locations) \n",
    "    ratio=most_frequent/total \n",
    "    return format(ratio,'.3f')\n",
    "def checkCommonPageRatioinFooter(HTML,URL):\n",
    "    soup = BeautifulSoup(HTML, 'html.parser')\n",
    "    foot = soup.footer\n",
    "    #print('foot',foot)\n",
    "    if foot is None:\n",
    "        return 0\n",
    "    images = foot.findAll(\"img\")\n",
    "    links = foot.findAll(\"link\")\n",
    "    anchors = foot.findAll(\"a\")\n",
    "    sounds = foot.findAll(\"sound\")\n",
    "    videos = foot.findAll(\"video\")\n",
    "    metas = foot.findAll(\"meta\")\n",
    "    li=foot.findAll('li')\n",
    "    scripts = foot.findAll(\"script\")\n",
    "    objects = images + links + anchors + sounds + videos + metas + scripts+li\n",
    "    if (len(objects) == 0):\n",
    "        # print('no objects')\n",
    "        return 0  # no embedded objects in html\n",
    "    object_locations = []\n",
    "    ex_domains = []\n",
    "    frequency_in = 0\n",
    "    for o in objects:\n",
    "        try:\n",
    "            object_location = o['src']\n",
    "            object_locations.append(object_location)\n",
    "        except:\n",
    "            try:\n",
    "                object_location = o['href']\n",
    "                object_locations.append(object_location)\n",
    "            except:\n",
    "                continue\n",
    "        if isInternal(object_location, URL):\n",
    "            frequency_in = frequency_in + 1\n",
    "        else:\n",
    "            ex_domain = ((urlparse(object_location)).netloc).lower()\n",
    "            ex_domains.append(ex_domain)\n",
    "    #print('object_locations', object_locations)\n",
    "    if (len(object_locations) == 0):\n",
    "        # print('no objects')\n",
    "        return 0  # no embedded url in html\n",
    "    #print('ex_domains', ex_domains)\n",
    "    if len(ex_domains) > 0:\n",
    "        try:\n",
    "            frequency_ex = max(ex_domains.count(b) for b in ex_domains if b)\n",
    "        except:\n",
    "            frequency_ex = 0\n",
    "    else:\n",
    "        frequency_ex = 0\n",
    "\n",
    "    #print('frequency_ex', frequency_ex)\n",
    "    #print('frequency_in is', frequency_in)\n",
    "    if frequency_in >= frequency_ex:\n",
    "        most_frequent = frequency_in\n",
    "    else:\n",
    "        most_frequent = frequency_ex\n",
    "    #print('most_frequent',most_frequent)\n",
    "    total = len(object_locations)\n",
    "    #print('total',total)\n",
    "    ratio = most_frequent / total\n",
    "    return format(ratio, '.3f')\n",
    "def checkSFH(HTML, URL):\n",
    "    '''\n",
    "    Function that checks how many forms are suspicious.\n",
    "    The return value depends on the rate of suspicious FORMS, which is compared against 2 thresholds (suspicious (0) and phising (1)).\n",
    "    '''\n",
    "    suspicious_threshold = 0.5\n",
    "    phishing_threshold = 0.75\n",
    "    soup = BeautifulSoup(HTML, \"html.parser\")\n",
    "    forms = soup.findAll(\"form\")\n",
    "    if(len(forms)==0):\n",
    "        return -1 #no forms in html\n",
    "    suspicious_forms = []\n",
    "    for f in forms:\n",
    "        try:\n",
    "            form_location = f['action']\n",
    "        except:\n",
    "            continue\n",
    "        if(not(isInternal(form_location, URL))):\n",
    "            suspicious_forms.append(f)\n",
    "        elif(form_location==\"about:blank\"):\n",
    "            suspicious_forms.append(f)\n",
    "        elif(form_location==\"\"):\n",
    "            suspicious_forms.append(f)\n",
    "    suspicious_forms_rate = len(suspicious_forms)/len(forms)\n",
    "    \n",
    "    if(suspicious_forms_rate < suspicious_threshold):\n",
    "        return -1\n",
    "    elif(suspicious_forms_rate < phishing_threshold):\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "def checkPopUp(HTML):\n",
    "    '''\n",
    "    Function that checks if the HTML contains code that triggers a popup window with input text fields.\n",
    "    These elements are introduced with the \"prompt()\" code. Other popup windows can be introduced with the code \"window.open()\".\n",
    "    This function returns 1 if the HTML contains popup windows with text fields; 0 if it contains any popup window; and -1 if no popup windows are found.\n",
    "    '''\n",
    "    if(HTML.find(\"prompt(\")>=0):#input data\n",
    "        return 1\n",
    "    elif(HTML.find(\"window.open(\")>=0):\n",
    "        return 0\n",
    "    return -1\n",
    "def checkRightClick(HTML):\n",
    "    '''\n",
    "    Function that inspects the provided HTML to determine if the CONTEXTMENU has been disabled (which is the equivalent of disabling the mouse right click)\n",
    "    This can be performed in several ways.\n",
    "    It returns 1 if the contextmenu is disabled, and -1 otherwise.\n",
    "    '''\n",
    "    contextmenu_disabler_JS = \"preventDefault()\"\n",
    "    contextmenu_disabler_html = 'oncontextmenu=\"return false;\"'#;\n",
    "\n",
    "    if(HTML.find(contextmenu_disabler_html)>=0):\n",
    "        #print(\"found oncontextmenu\")\n",
    "        return 1\n",
    "#     elif(HTML.find(contextmenu_disabler_JS)>=0):\n",
    "#         print(\"found preventDefault\")\n",
    "#         return 1\n",
    "    return -1\n",
    "def nullLinksinWeb(HTML, URL):\n",
    "    '''\n",
    "    Function that checks how many suspicious anchors are contained in a website.\n",
    "    The return value depends on the number of  suspicious anchors\n",
    "    ''' \n",
    "    soup = BeautifulSoup(HTML, \"html.parser\") \n",
    "    anchors = soup.findAll(\"a\")\n",
    "    if(len(anchors)==0):\n",
    "        return 0 #no anchors in html\n",
    "    suspicious_anchors = []\n",
    "    for a in anchors:\n",
    "        try:\n",
    "            anchor_location = a['href']\n",
    "        except:\n",
    "            continue\n",
    "        if(anchor_location == \"#\"):\n",
    "            suspicious_anchors.append(a)\n",
    "        elif(anchor_location == \"#content\"):\n",
    "            suspicious_anchors.append(a)\n",
    "        elif(anchor_location == \"#skip\"):\n",
    "            suspicious_anchors.append(a)\n",
    "        elif(anchor_location == \"JavaScript ::void(0)\"):\n",
    "            suspicious_anchors.append(a)\n",
    "        elif((isInternal(anchor_location, URL))):\n",
    "            suspicious_anchors.append(a)\n",
    "    try:\n",
    "        suspicious_anchors_rate = len(suspicious_anchors)/len(anchors)\n",
    "    except:\n",
    "        return 0\n",
    "     #print(suspicious_anchors)  \n",
    "    #print('suspicious_anchors_rate',suspicious_anchors_rate)\n",
    "    return format(suspicious_anchors_rate,'.2f')\n",
    "'''\n",
    "check the null value in the footer\n",
    "Most of the legitimate websites do not often use Null links in the footer section of a website, but the phishing sites\n",
    "usually insert null links in the footer section to make the user stay on the same page until the sensitive\n",
    "information is submitted by the user,calculate suspicious_anchor/anchors\n",
    "'''\n",
    "def nullLinksinFooter(HTML, URL):\n",
    "    \n",
    "    soup = BeautifulSoup(HTML, \"html.parser\")\n",
    "    foot = soup.footer\n",
    "    #print('foot',foot)\n",
    "    suspicious_anchors=[]\n",
    "    if foot is None:\n",
    "        return 0\n",
    "    anchors = foot.findAll(\"a\")\n",
    "    if (len(anchors)==0):\n",
    "        return 0\n",
    "    \n",
    "    for a in anchors:\n",
    "        try:\n",
    "            anchor_location = a['href']\n",
    "        except:\n",
    "            continue\n",
    "        if(anchor_location == \"#\"):\n",
    "            suspicious_anchors.append(a)\n",
    "        elif(anchor_location == \"#content\"):\n",
    "            suspicious_anchors.append(a)\n",
    "        elif(anchor_location == \"#skip\"):\n",
    "            suspicious_anchors.append(a)\n",
    "        elif(anchor_location == \"JavaScript ::void(0)\"):\n",
    "            suspicious_anchors.append(a)\n",
    "        #elif((isInternal(anchor_location, URL))):\n",
    "            #suspicious_anchors.append(a)\n",
    "    suspicious_anchors_rate = len(suspicious_anchors)/len(anchors) \n",
    "    ''' \n",
    "    print('foot',foot)\n",
    "    print('suspicious anchors',suspicious_anchors)\n",
    "    print('len of suspicious anchors',len(suspicious_anchors))\n",
    "    print('len of anchors',len(anchors))\n",
    "    print('suspicious_anchors_rate',suspicious_anchors_rate)\n",
    "    '''\n",
    "    return format(suspicious_anchors_rate,'.2f')\n",
    "def checkBrokenLink(HTML,URL):\n",
    "    '''\n",
    "    This feature extracts ratio of Not found links to the total number of links in a website. In legitimate sites, when all the links are\n",
    "connected either they return 200 Ok HTTP status code indicating server has accepted the request or sends 404\n",
    "status code indicating page not found in that server.broken_anchor/total_anchor\n",
    "    '''\n",
    "    soup = BeautifulSoup(HTML, \"html.parser\")\n",
    "    #get the frequency of external domain    \n",
    "    images = soup.findAll(\"img\")\n",
    "    links = soup.findAll(\"link\")\n",
    "    anchors = soup.findAll(\"a\")\n",
    "    sounds = soup.findAll(\"sound\")\n",
    "    videos = soup.findAll(\"video\")\n",
    "    metas = soup.findAll(\"meta\")\n",
    "    scripts = soup.findAll(\"script\")\n",
    "    objects = images + links + anchors + sounds + videos+metas + scripts\n",
    "    broken_link=0\n",
    "    if(len(objects)==0):\n",
    "        #print('no objects')\n",
    "        return 0 #no embedded objects in html\n",
    "    object_locations = []\n",
    "    for o in objects:\n",
    "        try:\n",
    "            object_location = o['src']\n",
    "            \n",
    "            if not(isInternal(object_location, URL)):\n",
    "                object_locations.append(object_location)\n",
    "            #print('object_location',object_location)\n",
    "            #print('compare',isInternal(object_location, URL))\n",
    "        except:\n",
    "            try:\n",
    "                object_location = o['href']\n",
    "                #print('object_location',object_location)\n",
    "                if not(isInternal(object_location, URL)):\n",
    "                    object_locations.append(object_location)\n",
    "                #print('compare',isInternal(object_location, URL))\n",
    "            except:\n",
    "                continue \n",
    "    #print('object_locations',object_locations)\n",
    "     \n",
    "    if(len(object_locations)==0):\n",
    "\n",
    "        return 0 #no embedded url in html\n",
    "    for obj in object_locations:\n",
    "         \n",
    "        try:\n",
    "            resp=urllib.request.urlopen(obj,timeout=2)\n",
    "            #print('resp',resp)\n",
    "            status_code=resp.getcode()\n",
    "#             print('status_code',status_code)\n",
    "            if status_code>=400: \n",
    "                broken_link=broken_link+1\n",
    "        except:\n",
    "            broken_link=broken_link+1           \n",
    "#     print('broken_link',broken_link)\n",
    "#     print('len object_location',len(object_locations))\n",
    "    broken_link_rate=broken_link/len(object_locations)\n",
    "#     print('broken_link_rate is',broken_link_rate)\n",
    "    return format(broken_link_rate,'.2f')  #0/1\n",
    "\n",
    "def checkLoginForm(HTML,URL):\n",
    "    soup = BeautifulSoup(HTML, \"html.parser\")\n",
    "    #get the frequency of external domain    \n",
    "    forms = soup.findAll(\"form\")\n",
    "    empty=[\"\", \"#\", \"#nothing\", \"#doesnotexist\",\"#null\", \"#void\", \"#whatever\", \"#content\", \"javascript::void(0)\",\"javascript::void(0);\", \"javascript::;\", \"javascript\"]\n",
    "    try:\n",
    "        for obj in forms:\n",
    "            #print('obj')\n",
    "            for em in empty:\n",
    "                try:\n",
    "                    if obj['action']==em:\n",
    "                        return 1\n",
    "                except:\n",
    "                    return -1\n",
    "            if (not(isInternal(obj['action'],URL))):\n",
    "                return 1\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "    return -1 \n",
    "def checkHiddenInfo_div(HTML):\n",
    "    '''\n",
    "    Some special codes inHTML codes can prevent the content from displaying or restricting the function of a tag, \n",
    "    which may be used by phishing webpages. These special codes work on specific tags, \n",
    "    1.<div>:<div style=\"visibility:hidden\",<div style=\"display:none\">\n",
    "    2.<button disabled='disabled'>\n",
    "    3.<input type=hidden><input disabled='diabled'><input value='hello'> fills in some irrelevant info in the input box\n",
    "    '''\n",
    "    soup = BeautifulSoup(HTML, \"html.parser\")\n",
    "    #get the frequency of external domain    \n",
    "    divs = soup.findAll(\"div\")\n",
    "    #print('divs',divs)\n",
    "    for div in divs:\n",
    "        #print('div',div)\n",
    "        try:\n",
    "            if div['style']=='visibility:hidden' or div['style']=='display:none':\n",
    "                return 1\n",
    "        except:\n",
    "            return -1\n",
    "    return -1\n",
    "def checkHiddenInfo_button(HTML):\n",
    "    soup = BeautifulSoup(HTML, \"html.parser\")\n",
    "    buttons = soup.findAll(\"button\")\n",
    "    for button in buttons:\n",
    "        #print('button',button)\n",
    "        try:\n",
    "            if button['disabled']=='disabled':\n",
    "                return 1\n",
    "        except:\n",
    "            return -1\n",
    "    return -1\n",
    "def checkHiddenInfo_input(HTML):\n",
    "    soup = BeautifulSoup(HTML, \"html.parser\")\n",
    "    inputs = soup.findAll(\"input\")\n",
    "    #print('inputs',inputs)\n",
    "    for inp in inputs:\n",
    "        #print('inp',inp)\n",
    "        try:\n",
    "            if (inp[\"type\"]==\"hidden\"):\n",
    "                #print('hidden input is',inp)\n",
    "                return 1\n",
    "            if(inp.find('disabled')>-1):\n",
    "                #print('disabled input',inp)\n",
    "                return 1\n",
    "        except:\n",
    "            \n",
    "            return -1\n",
    "            \n",
    "    return -1\n",
    "\n",
    "    \n",
    "def checkIFrame(HTML):\n",
    "    soup=BeautifulSoup(HTML,'html.parser') \n",
    "    iframes=soup.find_all('iframe')\n",
    "    #print('iframes:',iframes)\n",
    "    for iframe in iframes:\n",
    "        try:\n",
    "            if (iframe['style'].find('display: none')>-1) or (iframe['style'].find('border: 0')>-1) or  (iframe['style'].find(\"visibility: hidden;\")>-1) or (iframe['frameborder'].find('0')>-1):\n",
    "                #print('shoule delete:',iframe)\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        except:\n",
    "            return -1\n",
    "    return -1\n",
    "def checkFavicon(HTML, URL):\n",
    "    '''\n",
    "    Function that determines if the Favicon of the website comes from an external source.\n",
    "    It returns 1 if it's from an external source; 0 if it does not have a Favicon. And -1 if the Favicon is internal.\n",
    "    '''\n",
    "   \n",
    "    soup = BeautifulSoup(HTML, \"html.parser\")\n",
    "    favicon = soup.find(rel=\"shortcut icon\")\n",
    "    #print('favicon:',favicon)\n",
    "    if (not favicon):\n",
    "        favicon = soup.find(rel=\"icon\")\n",
    "        #print('favicon:',favicon)\n",
    "    if favicon:\n",
    "        try:\n",
    "            favicon_location = favicon['href']\n",
    "            #print('favicon_location',favicon_location)\n",
    "        except:\n",
    "            #print('no favicon href')\n",
    "            return 0\n",
    "    else:\n",
    "        #print('no favicon')\n",
    "        #print(\"Cannot find Favicon for {}\".format(URL))\n",
    "        return 0\n",
    "    if(isInternal(favicon_location, URL)):\n",
    "        #print(\"Favicon Internal: {} - {}\".format(favicon_location, URL))\n",
    "        return -1\n",
    "    else:\n",
    "        #print('external favicon')\n",
    "        #print(\"Favicon External: {} - {}\".format(favicon_location, URL))\n",
    "        return 1\n",
    "def checkStatusBar(HTML):\n",
    "    '''\n",
    "    Function that inspects the provided HTML to determine if it changes the text of the statusbar.\n",
    "    It returns 1 if statusbar modifications are detected, and -1 otherwise.\n",
    "    '''\n",
    "    status_bar_modification = \"window.status\"\n",
    "    if(HTML.find(status_bar_modification)>=0):\n",
    "        return 1\n",
    "    return -1\n",
    "def checkCSS(HTML, URL):\n",
    "    '''\n",
    "    Function that determines if the CSS of the website comes from an external source.\n",
    "    It returns 1 if it's from an external source; and -1 otherwise.\n",
    "    ''' \n",
    "    soup = BeautifulSoup(HTML, \"html.parser\")\n",
    "    css = soup.find(rel=\"stylesheet\")\n",
    "    if css:\n",
    "        try:\n",
    "            css_location = css['href']\n",
    "        except:\n",
    "            #print(\"Cannot find linked stylesheet for {}\".format(URL))\n",
    "            return -1\n",
    "    else:\n",
    "        #print(\"Cannot find stylesheet for {}\".format(URL))\n",
    "        return -1\n",
    "    if(isInternal(css_location, URL)):\n",
    "        #print(\"Linked CSS Internal: {} - {}\".format(css_location, URL))\n",
    "        return -1\n",
    "    else:\n",
    "        #print(\"Linked CSS External: {} - {}\".format(css_location, URL))\n",
    "        return 1\n",
    "def checkAnchors(HTML, URL):\n",
    "    '''\n",
    "    Function that checks how many suspicious anchors are contained in a website.\n",
    "    The return value depends on the rate of suspicious anchors, which is compared against 2 thresholds (suspicious (0) and phising (-1)).\n",
    "    '''\n",
    "    \n",
    "    suspicious_threshold = 0.32\n",
    "    phishing_threshold = 0.505\n",
    "    soup = BeautifulSoup(HTML, \"html.parser\")\n",
    "    anchors = soup.findAll(\"a\")\n",
    "    if(len(anchors)==0):\n",
    "        return -1 #no anchors in html\n",
    "    suspicious_anchors = []\n",
    "    for a in anchors:\n",
    "        try:\n",
    "            anchor_location = a['href']\n",
    "        except:\n",
    "            continue\n",
    "        if(anchor_location == \"#\"):\n",
    "            suspicious_anchors.append(a)\n",
    "        elif(anchor_location == \"#content\"):\n",
    "            suspicious_anchors.append(a)\n",
    "        elif(anchor_location == \"#skip\"):\n",
    "            suspicious_anchors.append(a)\n",
    "        elif(anchor_location == \"JavaScript ::void(0)\"):\n",
    "            suspicious_anchors.append(a)\n",
    "        elif(not(isInternal(anchor_location, URL))):\n",
    "            suspicious_anchors.append(a)\n",
    "    suspicious_anchors_rate = len(suspicious_anchors)/len(anchors)\n",
    "    '''\n",
    "    print('suspicious anchors:',suspicious_anchors)\n",
    "    print('anchors are',anchors)\n",
    "    print('length of suspicious anchors:',len(suspicious_anchors))\n",
    "    print('length of anchors:',len(anchors))\n",
    "    print('suspicious_anchors_rate',suspicious_anchors_rate)\n",
    "    '''\n",
    "    if(suspicious_anchors_rate < suspicious_threshold):\n",
    "        return -1\n",
    "    elif(suspicious_anchors_rate < phishing_threshold):\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d199997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis feature examines whether a website is in Google’s index or not. \\nWhen a site is indexed by Google, it is displayed on search results (Webmaster resources, 2014),if the search result is none->phishing\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\nchack page rank 1-10,Phishing web pages are not very popular, hence, they suppose to have low page\\nranks compared with legitimate web pages. We use Openpagerank to get the value of this feature.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\n===================get whois info\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\n    Function that checks if the domain age of the website is suspicious or not.\\n    It considers the length of the period between the domain creation and its expiration dates (from the WHOIS query).\\n    It returns 1 if the length cannot be computed, or if the length is shorter than \"age_threshold\"; and -1 otherwise.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\n    Function that checks if the domain age of the website is suspicious or not.\\n    It considers the length of the period between the domain creation and today (from the WHOIS query).\\n    It returns 1 if the length cannot be computed, or if the length is shorter than \"age_threshold\"; and -1 otherwise.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\n    Function that checks if the website has the proper configuration of ports.\\n    It considers the status of ports: 21, 22, 23, 80, 443, 445, 1433, 1521, 3306, 3389.\\n    state: ports open is 0,close is:1\\n    It returns -1 if at most 2 ports are not of the preferred status; 0 if at most 5 ports are not of the preferred status; and 1 otherwise.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Third party service features\n",
    "\n",
    "'''\n",
    "This feature examines whether a website is in Google’s index or not. \n",
    "When a site is indexed by Google, it is displayed on search results (Webmaster resources, 2014),if the search result is none->phishing\n",
    "'''\n",
    "def checkGI(URL):\n",
    "    try:\n",
    "        domain=urlparse(URL).netloc\n",
    "    except:\n",
    "        return 1\n",
    "    API_KEY = \"AIzaSyAtegN2m50mIN4wBgS1vpucFHNL7M7OH3E\"\n",
    "    SEARCH_ENGINE_ID = \"0e2567514cd3e419a\"\n",
    "    query = domain\n",
    "    page = 1\n",
    "    start = (page - 1) * 10 + 1\n",
    "    url = f\"https://www.googleapis.com/customsearch/v1?key={API_KEY}&cx={SEARCH_ENGINE_ID}&q={query}&start={start}\"\n",
    "    data = requests.get(url).json()\n",
    "    #print('checkgi data',data)\n",
    "    #print(len(data))\n",
    "    try:\n",
    "        search_items = data.get(\"items\")\n",
    "        if search_items==None:\n",
    "            return 1\n",
    "    except:\n",
    "        return 1\n",
    "     \n",
    "    return -1 \n",
    "'''\n",
    "chack page rank 1-10,Phishing web pages are not very popular, hence, they suppose to have low page\n",
    "ranks compared with legitimate web pages. We use Openpagerank to get the value of this feature.\n",
    "''' \n",
    "def checkPR(URL):\n",
    "    try:\n",
    "        domain=get_fld(URL)\n",
    "    except:\n",
    "        return 0\n",
    "    #print('domain',domain) \n",
    "    headers = {'API-OPR':'c48080g840k0wc8cw88g0o40w4gg4kcksgs00k8k'}\n",
    "    url = 'https://openpagerank.com/api/v1.0/getPageRank?domains%5B0%5D=' + domain\n",
    "    request = requests.get(url, headers=headers)\n",
    "    result = request.json()\n",
    "   \n",
    "    try:\n",
    "        resp=result['response']#[\"page_rank_integer\"]\n",
    "    except:\n",
    "        return 0\n",
    "    #print('resp',resp)\n",
    "    for item in resp:\n",
    "        pr=item['page_rank_integer']\n",
    "    #print(\"pr is\",pr)\n",
    "    return pr\n",
    "'''\n",
    "===================get whois info\n",
    "'''\n",
    "#get whois_info\n",
    "\n",
    "def getWhois(URL):\n",
    "    try:\n",
    "        who=whois.whois(URL)\n",
    "        #print('who is',who)\n",
    "    except:\n",
    "        who=None\n",
    "    return who\n",
    "\n",
    "def checkDNS(who):\n",
    "    '''\n",
    "    Function that inspects the whois of a website.\n",
    "    It returns -1 if the whois is found, and 1 otherwise\n",
    "    '''\n",
    "    if(who==None):\n",
    "        return 1\n",
    "    try:\n",
    "        domain_name = who['domain_name']\n",
    "    except:\n",
    "        try:\n",
    "            domain_name = who['domain']\n",
    "        except:\n",
    "            return 1\n",
    "    if(domain_name) is None:\n",
    "        return 1\n",
    "    return -1\n",
    "#get the age of domain from whois dataset,input url  \n",
    "'''\n",
    "    Function that checks if the domain age of the website is suspicious or not.\n",
    "    It considers the length of the period between the domain creation and its expiration dates (from the WHOIS query).\n",
    "    It returns 1 if the length cannot be computed, or if the length is shorter than \"age_threshold\"; and -1 otherwise.\n",
    "'''\n",
    "def checkRegistrationLen(who):#domain\n",
    "    age_threshold = 364\n",
    "    if(who==None):\n",
    "        return 1\n",
    "    try:\n",
    "\n",
    "        #info=whois.whois(domain)\n",
    "        creation=who['creation_date'][0]\n",
    "        expiration =who['expiration_date'][0]\n",
    "    except:\n",
    "        return 1\n",
    "    \n",
    "    length =(expiration-creation).days\n",
    "    #print(length)\n",
    "    if (length > age_threshold):\n",
    "        return -1\n",
    "    return 1\n",
    "\n",
    "'''\n",
    "    Function that checks if the domain age of the website is suspicious or not.\n",
    "    It considers the length of the period between the domain creation and today (from the WHOIS query).\n",
    "    It returns 1 if the length cannot be computed, or if the length is shorter than \"age_threshold\"; and -1 otherwise.\n",
    "'''\n",
    "#6 months\n",
    "def checkAge(URL,who):#domain\n",
    "    if(who==None):\n",
    "        return 1\n",
    "    age_threshold = 180\n",
    "    try: \n",
    "        #print('ho',who)\n",
    "        creation=who['creation_date'][0]#1999-10-11 11:05:17\n",
    "        #print('creation',creation)\n",
    "        now=datetime.now()\n",
    "        #print('now',now)\n",
    "    except:\n",
    "        return 1\n",
    "    try:\n",
    "        age =(now-creation).days\n",
    "        if (age > age_threshold):\n",
    "            return -1\n",
    "    except:\n",
    "        return 2\n",
    "    #print(age) \n",
    "    return 1\n",
    "#check abnormal url,if host name isn't in the url,->phishing\n",
    "def checkAbnormal(who, URL):\n",
    "    '''\n",
    "    Function that compares the identity of a website with the record of the whois database.\n",
    "    It returns 1 if these two do not match, and -1 otherwise.    \n",
    "    '''\n",
    "     \n",
    "    if(URL.count('.')==1 and URL.startswith('http')==False):\n",
    "        ind=URL.find('/')\n",
    "        if ind>-1:\n",
    "            domain=URL[:ind]\n",
    "        else:\n",
    "            domain = URL\n",
    "    else:\n",
    "        domain = ((urlparse(URL)).netloc).lower()\n",
    "    if(whois==None):\n",
    "        return 1\n",
    "    try:\n",
    "        domain_name = who['domain_name']\n",
    "    except:\n",
    "        try:\n",
    "            domain_name = who['domain']\n",
    "        except:\n",
    "            return 1 #no domain   \n",
    "    if(domain_name) is None:\n",
    "        return 1    \n",
    "    if (len(domain_name[0])==1):\n",
    "        #single domain\n",
    "        if(domain==(domain_name.lower())):\n",
    "            return -1\n",
    "    else:\n",
    "        #multiple domains\n",
    "        for d in domain_name:\n",
    "            if(domain==d.lower()):\n",
    "                return -1    \n",
    "    return 1\n",
    "\n",
    "#check ports,using nmap\n",
    "'''\n",
    "    Function that checks if the website has the proper configuration of ports.\n",
    "    It considers the status of ports: 21, 22, 23, 80, 443, 445, 1433, 1521, 3306, 3389.\n",
    "    state: ports open is 0,close is:1\n",
    "    It returns -1 if at most 2 ports are not of the preferred status; 0 if at most 5 ports are not of the preferred status; and 1 otherwise.\n",
    "'''\n",
    "def checkPorts(URL):\n",
    "    try:\n",
    "        domain=get_fld(URL)\n",
    "    except:\n",
    "        return 1\n",
    "    suspicious_threshold = 5\n",
    "    legitimate_threshold = 8\n",
    "    preferred_status={\"21\":\"close\",\n",
    "                      \"22\":\"close\",\n",
    "                      \"23\":\"close\",\n",
    "                      \"80\":\"open\",\n",
    "                      \"443\":\"open\",\n",
    "                      \"445\":\"close\",\n",
    "                      \"1433\":\"close\",\n",
    "                      \"1521\":\"close\",\n",
    "                      \"3306\":\"close\",\n",
    "                      \"3389\":\"close\"  \n",
    "                     }         \n",
    "    try:\n",
    "        domain_str=\"nmap %s -p21,22,23,80,443,445,1433,1521,3306,3389\"%domain\n",
    "        return_code,output=subprocess.getstatusoutput(domain_str)\n",
    "        #print('output',output)\n",
    "        first=output.find('21/tcp')\n",
    "        #print('first',first)\n",
    "        last=output.find('ms-wbt-server')\n",
    "        #print('last',last)\n",
    "        out=output[first:last]\n",
    "        lines=out.splitlines()\n",
    "        #print('lines:',lines)\n",
    "        same=0\n",
    "        num=0\n",
    "        try:\n",
    "            for key in preferred_status.keys():\n",
    "                if (lines[num].find(preferred_status.get(key)))>-1:\n",
    "                    same=same+1\n",
    "                num=num+1\n",
    "                \n",
    "        except:\n",
    "            num=0\n",
    "       # print('===========same',same)\n",
    "        if same>=legitimate_threshold:\n",
    "            return -1\n",
    "        elif same>=suspicious_threshold:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    except Exception as e:\n",
    "            logger = logging.getLogger(__name__)\n",
    "            logger.error(u'error:%s'%e)\n",
    "    return 1\n",
    "#check the ssl_certificate,input the domain, \n",
    "#if the url using  https, issuer is ture, and age of certificate>1 year->legit,phishy otherwise\n",
    "#google.com ssl 1 ago and in 1month\n",
    "#baidu:10months ago and in 2 month\n",
    "def checkSSL(URL):\n",
    "    \n",
    "    #print('url is',URL)\n",
    "    try:\n",
    "        domain=get_fld(URL)\n",
    "    except:\n",
    "        return 1\n",
    "    start_date=''\n",
    "    expire_date=''\n",
    "    cn=''\n",
    "    trusted_CAs = ['GeoTrust', 'GoDaddy', 'Network Solutions', 'Thawte', 'Comodo', 'Doster', 'Verisign', 'SSL.com', 'Secure128', \n",
    "                   'Google', 'InCommon', 'Trustico', 'GlobalSign', 'SSLRenewals', 'DigiCert', 'Symantec', 'IdenTrust', \n",
    "                   'EnTrust', 'RapidSSL', 'Encrypt', 'Amazon', 'D-Trust', 'Starfield', 'Gandi', 'Sectigo', 'Microsoft', \n",
    "                   'WellsFargo', 'GoGetSSL', 'QuoVadis', 'Trusted Secure Certificate Authority', 'Certum', 'TrustWave', 'TeleSec', \n",
    "                   'CyberTrust Japan', 'DFN-Verein', 'Actalis', 'SwissSign', 'Apple', 'Affirm', 'SecureCore', 'Strato', 'DonDominio',\n",
    "                   'Globe', 'Gehirn']  \n",
    "    \n",
    "    duration_threshold =300 #299\n",
    "    trusted = False \n",
    "    try:\n",
    "        domain_str=\"curl -Ivs https://%s --connect-timeout 10\"%domain#.encode('UTF-8')\n",
    "        return_code,output=subprocess.getstatusoutput(domain_str)\n",
    "        #print('domain_str is',domain_str) \n",
    "        #print('out is',output)\n",
    "        m=re.search('SSL connection using (.*?)\\n.*?start date: (.*?)\\n.*?expire date: (.*?)\\n.*?issuer: (.*?)\\n.*?',output,re.S)\n",
    "        if m: \n",
    "            start_date=m.groups()[1] \n",
    "            expire_date=m.groups()[2] \n",
    "            issuer=m.groups()[3] \n",
    "            #time.strptime\n",
    "            start_date=datetime.strptime(start_date,\"%b %d %H:%M:%S %Y %Z\")\n",
    "            expire_date=datetime.strptime(expire_date,\"%b %d %H:%M:%S %Y %Z\")\n",
    "            length=(expire_date-start_date).days\n",
    "            #print('length',length) \n",
    "            dic={i.split(\"=\")[0]:i.split(\"=\")[1] for i in issuer.split(\";\")} \n",
    "            cn=dic[' CN']\n",
    "            #print('dic is',dic)\n",
    "            #check if the ca is trusted\n",
    "            try:\n",
    "                issuer=cn.lower()\n",
    "                for tCA in trusted_CAs:\n",
    "                    if (issuer.find(tCA.lower())>-1):\n",
    "                        trusted=True\n",
    "            except:\n",
    "                issuer=0\n",
    "            \n",
    "            if(trusted==True and length>duration_threshold): \n",
    "                return -1 #legit: long length\n",
    "            else: \n",
    "                return 1        \n",
    "    except Exception as e:\n",
    "            logger = logging.getLogger(__name__)\n",
    "            logger.error(u'error:%s'%e)\n",
    "    return 1\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5d32b0",
   "metadata": {},
   "source": [
    "# 5 chinese-specific features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "592b3c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#checking if the icp info of the webpage consistent with info on icp system\n",
    "def tian_icp(info):\n",
    "    result_list={}\n",
    "    conn = http.client.HTTPSConnection('api.tianapi.com') #api\n",
    "    params = urllib.parse.urlencode({'key':'1b6c3ce66a4bf85da0aa19d057de175a','domain':info})\n",
    "    headers = {'Content-type':'application/x-www-form-urlencoded'}\n",
    "    conn.request('POST','/icp/index',params,headers)\n",
    "    res = conn.getresponse()\n",
    "    data = res.read()\n",
    "    #print(data.decode('utf-8'))\n",
    "    resp=json.loads(data)\n",
    "#     print('resp is',resp)\n",
    "    #print('resp newslist is',resp['newslist'])\n",
    "    try:\n",
    "        result=resp['newslist']\n",
    "        result_list['host_name']=result[0]['main_name']\n",
    "        result_list['type']=result[0]['icp_type']\n",
    "        result_list['pass_date']=result[0]['update_time']\n",
    "        result_list['domain-name']=result[0]['domain']\n",
    "        result_list['beian_code']=result[0]['icp_number']\n",
    "        #print('result_list is',result_list)\n",
    "        #print('result_list',result_list['beian_code'])\n",
    "        #domain_info['beian_license']\n",
    "        return result_list\n",
    "    except:\n",
    "        print('result is none')\n",
    "        return None\n",
    "\n",
    "def tian_check_icp(result_list,html): \n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    try:\n",
    "        #beian_license=result_list['beian_license']\n",
    "        beian_code=result_list['beian_code']# query response\n",
    "#         print('beian_code from online',beian_code)\n",
    "        icps_list=get_html_icp(html)# list from html\n",
    "#         print('icps_list from html',icps_list)\n",
    "        \n",
    "        for icp in icps_list:\n",
    "            filter_icp=re.findall('\\d\\d+',icp)[0]\n",
    "            #print('filter icp',filter_icp)\n",
    "            #print('icp in beiancode',icp in beian_code)\n",
    "            #print('filter icp in beiancode',filter_icp in beian_code)\n",
    "            if icp in beian_code or filter_icp in beian_code:\n",
    "#                 print('consistense')\n",
    "                return 0# benign\n",
    "            else:\n",
    "                continue\n",
    "        return 1#no one is consistense- phish\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "def domain_applicant(result_list):\n",
    "    ''' whether the domain applicant is a company in icp system'''\n",
    "    if result_list is None:\n",
    "        return 1\n",
    "    applicant=result_list['type']\n",
    "    #print(applicant)\n",
    "    if applicant=='企业': #enterprise\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def domain_recoder(result_list,domain):\n",
    "    ''' whether the domain in icp system is consistent with the given webpage's domain '''\n",
    "    if result_list is None:\n",
    "        return 1\n",
    "    do=result_list['domain-name']\n",
    "    if do==domain:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def domain_register(result_list):\n",
    "    ''' icp whether the current domain registed in icp system'''\n",
    "    if result_list is None or len(result_list)==0:\n",
    "        return 1\n",
    "    host=result_list['host_name']\n",
    "    domain=result_list['domain-name']\n",
    "    #print('len of domain is',len(domain))\n",
    "    if len(domain)==0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def e_certificate(html):\n",
    "    ''' check if the webpage include e-commerce certificate link '''\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    links=soup.find_all('a',href=True)\n",
    "#http://cx.zw.cn/，cnnic\n",
    "    security_list=['credit','knet.cn','yunaq.com','bcpcn.com','cnnic.cn','internic.net','12377.cn','cyberpolice.cn','12321.cn','ec.org.cn']\n",
    "    for container in links:\n",
    "        href=container['href']\n",
    "        for word in security_list:\n",
    "            if word in href:\n",
    "#                 print('word%s in href %s'%(word,href))\n",
    "                images=container.findAll('img',recursive=False)\n",
    "                #print('len of images',len(images))\n",
    "                if len(images)>0:\n",
    "                    return 0 # have the serucity website images\n",
    "            else:\n",
    "                continue\n",
    "    return 1 #no mark- phish\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34733d0",
   "metadata": {},
   "source": [
    "# the other 5 new features, and revised 2 spacephish feature (H_titBr and H_ominCopr), refer to Appendix A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "720c9ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_chinese(string):\n",
    "    pre=re.compile(u'[\\u4e00-\\u9fa5]')\n",
    "    res=re.findall(pre,string)\n",
    "    res1=''.join(res)\n",
    "    return res1\n",
    "def convert_pinyin(string): #res1\n",
    "    p = Pinyin()\n",
    "    py=p.get_pinyin(string, '')\n",
    "    return py\n",
    "\n",
    "def checkTitleUrlBrand(HTML,URL): \n",
    "    try:\n",
    "        domain_brand =(get_tld(URL,as_object=True)).domain\n",
    "    except:\n",
    "        return 2 #suspicious, no valid\n",
    "    #print('doman brand is',domain_brand)\n",
    "    soup=BeautifulSoup(HTML,'html.parser')\n",
    "    try:\n",
    "        title=soup.find('title').get_text()\n",
    "        zh=find_chinese(title)\n",
    "        py=convert_pinyin(zh)\n",
    "        #print('title is',title)\n",
    "        tit=title.lower()\n",
    "        #print(domain_brand.lower() in tit or domain_brand.lower() in py.lower() or )\n",
    "        if domain_brand.lower() in tit or domain_brand.lower() in py.lower() or domain_brand.lower() in tit.replace(' ',''):\n",
    "            return 0 #benign\n",
    "        else:\n",
    "            return 1\n",
    "    except:\n",
    "        return 2# no valid\n",
    "\n",
    "def checkDomainwithCopyright(HTML,URL):\n",
    "    try:\n",
    "        res=get_tld(URL, as_object=True)\n",
    "    except:\n",
    "        return 1\n",
    "    domain=res.domain\n",
    "    #print('domain is:',domain)\n",
    "    soup = BeautifulSoup(HTML,'html.parser')\n",
    "    \n",
    "    for text in soup.stripped_strings:\n",
    "         if '©' in text or 'Copyright' in text:\n",
    "            text = re.sub(r'\\s+', ' ', text)  # condense any whitespace\n",
    "            #print('text is',text)\n",
    "            zh=find_chinese(text)\n",
    "            py=convert_pinyin(zh)\n",
    "            #print('py is',py)\n",
    "            #print(domain.lower() in text.lower() or domain.lower() in py)\n",
    "            te=text.lower()\n",
    "            if domain.lower() in te or domain.lower() in py or domain.lower() in te.replace(' ',''):               \n",
    "                return 0 \n",
    "    return 1\n",
    "\n",
    "def checkunicode(hostname):\n",
    "    ''' check if the url includes unicode, then phish'''\n",
    "    if hostname is None:\n",
    "        return 1\n",
    "    pattern = re.compile(r'[\\S]*(%[a-fA-F0-9]{2})+') \n",
    "    if pattern.match(hostname):\n",
    "        return 1\n",
    "    return 0 \n",
    "\n",
    "def count_suffixes(hostname):\n",
    "    '''\n",
    "    the count of tld \n",
    "    '''\n",
    "    if hostname is None:\n",
    "        return 0\n",
    "    #top_tld=['surf','cn','bid','gq','ml','cf','work','cam','ga','casa','tk','ga','top','cyou','bar','rest']\n",
    "    counts=[]   \n",
    "    tld_list=['.cn','.com','.top','.org','.net','.us','.uk','.jp','.xyz','.ai','.edu','.gov','.int']\n",
    "    for tld in tld_list:\n",
    "        number=hostname.count(tld)\n",
    "        counts.append(number) \n",
    "    return max(counts) \n",
    "\n",
    "\n",
    "def get_domain(url):\n",
    "    '''extract the domain, hostname and path from url'''\n",
    "    f_domain=get_fld(url)\n",
    "#     print('f_domain is',f_domain)\n",
    "    o=urlsplit(url)\n",
    "#     print('o is',o)\n",
    "#     print('o.hostname %s,f_domain %s ,o.pathis %s'%(o.hostname,f_domain,o.path))\n",
    "    try:\n",
    "        f_domain=get_fld(url)\n",
    "        o=urlsplit(url)\n",
    "        #urllib.parse.\n",
    "        #print('hostname is %s,domain is %s,path is %s'%(o.hostname,f_domain,o.path))\n",
    "        return o.hostname,f_domain,o.path\n",
    "    except:\n",
    "        print('can not get the domain, hostname and path')\n",
    "        return None,None,None\n",
    "      \n",
    "def h_external(domain,hostname,images,links,anchors,audios,forms,csss):\n",
    "    ''' the number of external items '''\n",
    "    external_ob=[]\n",
    "    if hostname is None or domain is None:\n",
    "        return -1\n",
    "    else:   \n",
    "        for link in links:\n",
    "            dots = [x.start(0) for x in re.finditer('\\.', link['href'])]\n",
    "            if hostname in link['href'] or domain in link['href'] or len(dots) == 1 or not link['href'].startswith('http'):\n",
    "                continue\n",
    "            else:\n",
    "                external_ob.append(link['href'])\n",
    "        for anchor in anchors:\n",
    "            dots = [x.start(0) for x in re.finditer('\\.', anchor['href'])]\n",
    "            if hostname in anchor['href'] or domain in anchor['href'] or len(dots) == 1 or not anchor['href'].startswith('http'):\n",
    "                continue\n",
    "            else:\n",
    "                external_ob.append(anchor['href'])\n",
    "        for img in images:\n",
    "            dots = [x.start(0) for x in re.finditer('\\.', img['src'])]\n",
    "            if hostname in img['src'] or domain in img['src'] or len(dots) == 1 or not img['src'].startswith('http'):\n",
    "                continue\n",
    "            else:\n",
    "                external_ob.append(img['src'])\n",
    "        for audio in audios:\n",
    "            dots = [x.start(0) for x in re.finditer('\\.', audio['src'])]\n",
    "            if hostname in audio['src'] or domain in audio['src'] or len(dots) == 1 or not audio['src'].startswith('http'):\n",
    "                continue\n",
    "            else:\n",
    "                external_ob.append(audio['src'])\n",
    "        for form in forms:\n",
    "            dots = [x.start(0) for x in re.finditer('\\.', form['action'])]\n",
    "            if hostname in form['action'] or domain in form['action'] or len(dots) == 1 or not form['action'].startswith('http'):\n",
    "                continue\n",
    "            else:\n",
    "                external_ob.append(form['action'])\n",
    "\n",
    "        for style in csss:\n",
    "            try: \n",
    "                start = str(style[0]).index('@import url(')\n",
    "                end = str(style[0]).index(')')\n",
    "                css = str(style[0])[start+12:end]\n",
    "                dots = [x.start(0) for x in re.finditer('\\.', css)]\n",
    "                if hostname in css or domain in css or len(dots) == 1 or not css.startswith('http'):\n",
    "                    continue\n",
    "                else: \n",
    "                    external_ob.append(css)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        #print('length of external objects',len(external_ob))\n",
    "        return len(external_ob)\n",
    "def h_null(images,links,anchors,audios,forms,csss):\n",
    "    ''' the number of items that link targets current page or null items'''\n",
    "    null_ob=[]\n",
    "    Null_format = [\"\", \"#\", \"#nothing\", \"#doesnotexist\", \"#null\", \"#void\", \"#whatever\",\n",
    "               \"#content\", \"javascript::void(0)\", \"javascript::void(0);\", \"javascript::;\", \"javascript\",\"javascript:void(0)\",\"javascript:void(0);\"] \n",
    "    for link in links:\n",
    "        if link['href'] in Null_format:\n",
    "            null_ob.append(link)\n",
    "    for anchor in anchors:\n",
    "        if anchor['href'] in Null_format:\n",
    "            null_ob.append(anchor)\n",
    "    for audio in audios:\n",
    "        if audio['src'] in Null_format:\n",
    "            null_ob.append(audio)\n",
    "    for form in forms:\n",
    "        if form['action'] in Null_format:\n",
    "            null_ob.append(form)\n",
    "    for image in images:\n",
    "        if image['src'] in Null_format:\n",
    "            null_ob.append(image)\n",
    "    for style in csss:\n",
    "        try: \n",
    "            start = str(style[0]).index('@import url(')\n",
    "            end = str(style[0]).index(')')\n",
    "            css = str(style[0])[start+12:end]\n",
    "            dots = [x.start(0) for x in re.finditer('\\.', css)]\n",
    "            if hostname in css or domain in css or len(dots) == 1 or not css.startswith('http'):\n",
    "                if not css.startswith('http'):\n",
    "                    if css in Null_format:\n",
    "                        null_ob.append(css)\n",
    "        except:\n",
    "            continue\n",
    "    #print('length of null objects',len(null_ob))\n",
    "    return len(null_ob)\n",
    "#new one,\n",
    "# polish later\n",
    "def get_object(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    images = soup.findAll(\"img\",src=True)\n",
    "    links = soup.findAll(\"link\",href=True)#href, favicon\n",
    "    anchors = soup.findAll(\"a\",href=True)\n",
    "    audios = soup.findAll(\"audio\",src=True)\n",
    "    forms = soup.findAll(\"form\",action=True)\n",
    "    css=soup.find_all('style', type='text/css')\n",
    "    #objects = images + links + anchors+ audios+forms+css\n",
    "    return images,links,anchors,audios,forms,css  \n",
    "\n",
    "def checkSearchEngine(URL):\n",
    "    '''\n",
    "    using google,function check if the url's base domain matches the top 10 websites in google search result\n",
    "    if in,-1,else:1->phishing\n",
    "    '''\n",
    "    try:\n",
    "        domain=get_fld(URL)\n",
    "        #print('domain',domain)\n",
    "    except:\n",
    "        return 1\n",
    "    API_KEY = \"AIzaSyAtegN2m50mIN4wBgS1vpucFHNL7M7OH3E\"\n",
    "    SEARCH_ENGINE_ID = \"0e2567514cd3e419a\"\n",
    "    query = domain\n",
    "    page = 1\n",
    "    start = (page - 1) * 10 + 1\n",
    "    url = f\"https://www.googleapis.com/customsearch/v1?key={API_KEY}&cx={SEARCH_ENGINE_ID}&q={query}&start={start}\"\n",
    "    data = requests.get(url).json()\n",
    "    #print('checksearchengine',data)\n",
    "    try:\n",
    "        search_items = data.get(\"items\")\n",
    "    except:\n",
    "        return 1\n",
    "    if search_items ==None:\n",
    "        return 1\n",
    "    for i, search_item in enumerate(search_items, start=1):\n",
    "        #title = search_item.get(\"title\")\n",
    "        try:\n",
    "            link = search_item.get(\"link\")\n",
    "        except:\n",
    "            return 1\n",
    "        if (link.find(domain)>-1):\n",
    "            return -1 \n",
    "    return 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49220e6",
   "metadata": {},
   "source": [
    "# extractor features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17b726be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_json(json_file):\n",
    "    json_f=open(json_file).read() \n",
    "    json_info=json.loads(json_f)\n",
    "    json_data=pd.json_normalize(json_info)\n",
    "    return json_data\n",
    "\n",
    "def readHtmlFile(page_id):\n",
    "    path=\"../data/chiphish/\"+page_id+'.html'# chphish dataset\n",
    "     \n",
    "    try:\n",
    "        with open(path) as f: \n",
    "            HTML=f.read()\n",
    "            print('html success',page_id)\n",
    "    except:\n",
    "        try:\n",
    "            with open(path,encoding='gbk')as f:\n",
    "                HTML=f.read()\n",
    "                print('html_success',page_id)\n",
    "        except:\n",
    "            try:\n",
    "                with open(path,encoding='gbk')as f:\n",
    "                    HTML=f.read()\n",
    "                    print('html_success',page_id)\n",
    "            except:        \n",
    "                print('no this file',page_id)\n",
    "                return \"default\"\n",
    "    return HTML \n",
    "\n",
    "def extract(json_data,out_):\n",
    "    for i in range(1030,1031): #previous 709\n",
    "\n",
    "        info=json_data.iloc[i]\n",
    "        pid=info['id']\n",
    "        \n",
    "        HTML=readHtmlFile(pid)\n",
    "        URL=info['url']\n",
    "\n",
    "        #print('chspec_info is',chspec_info)\n",
    "        #print('url dash is',total_info['URL_dash'])\n",
    "        hostname,domain,path=get_domain(URL)\n",
    "        who=getWhois(URL)\n",
    "\n",
    "        features={}\n",
    "        features[\"URL_length\"] = checkLength(URL)\n",
    "        features['URL_IP']=checkIP(URL)\n",
    "        features[\"URL_redirect\"] = checkRedirect(URL)\n",
    "        features[\"URL_shortener\"] = checkShortener(URL)                                      \n",
    "        features[\"URL_subdomains\"] = checkSubdomains(URL)\n",
    "        features[\"URL_at\"] =checkAt(URL)                     \n",
    "\n",
    "        features[\"URL_dash\"] = checkDash(URL)\n",
    "\n",
    "        features[\"URL_numberofCommonTerms\"]=checkNumberofCommonTerms(URL)\n",
    "        features[\"URL_checkNumerical\"]=checkNumerical(URL)\n",
    "        features[\"URL_checkPathExtend\"]=checkPathExtend(URL)\n",
    "        features[\"URL_checkPunycode\"]=checkPunycode(URL)\n",
    "        features[\"URL_checkSensitiveWord\"]=checkSensitiveWord(URL)\n",
    "        features[\"URL_checkTLDinPath\"]=checkTLDinPath(URL)\n",
    "        features[\"URL_checkTLDinSub\"]=checkTLDinSub(URL)\n",
    "        features[\"URL_totalWordUrl\"]=totalWordUrl(URL)\n",
    "        features[\"URL_shortestWordUrl\"]=shortestWordUrl(URL)\n",
    "        features[\"URL_shortestWordHost\"]=shortestWordHost(URL)\n",
    "        features[\"URL_shortestWordPath\"]=shortestWordPath(URL)\n",
    "\n",
    "        features[\"URL_longestWordUrl\"]=longestWordUrl(URL)\n",
    "        features[\"URL_longestWordHost\"]=longestWordHost(URL)\n",
    "        features[\"URL_longestWordPath\"]=longestWordPath(URL)\n",
    "        features[\"URL_averageWordUrl\"]=averageWordUrl(URL)\n",
    "        features[\"URL_averageWordHost\"]=averageWordHost(URL)\n",
    "        features[\"URL_averageWordPath\"]=averageWordPath(URL)\n",
    "        features[\"URL_checkStatisticRe\"]=checkStatisticRe(URL)\n",
    "        features[\"REP_SearchEngine\"] = checkSearchEngine(URL)\n",
    "        features['REP_checkGI']=checkGI(URL)\n",
    "        features[\"REP_pageRank\"] = checkPR(URL)\n",
    "        features[\"REP_DNS\"] = checkDNS(who)\n",
    "        features[\"REP_registrationLen\"]=checkRegistrationLen(who)\n",
    "        features[\"REP_Age\"] = checkAge(URL,who) \n",
    "\n",
    "        features[\"REP_abnormal\"] = checkAbnormal(who, URL)\n",
    "        features[\"REP_ports\"] = checkPorts(URL)\n",
    "        features[\"REP_SSL\"] = checkSSL(URL)\n",
    "        features['url_unicode']=checkunicode(hostname) \n",
    "        features['tld_number']=count_suffixes(hostname)\n",
    "        result_list=tian_icp(domain)\n",
    "        features['icp_applicant']=domain_applicant(result_list)\n",
    "        features['icp_domain']=domain_recoder(result_list,domain)\n",
    "        features['icp_dregister']=domain_register(result_list)\n",
    "\n",
    "        # html\n",
    "\n",
    "        objects=getObjects(HTML)\n",
    "        features[\"HTML_Objects\"] = checkObjects(objects,HTML,URL)\n",
    "        features[\"HTML_metaScripts\"] = checkMetaScripts(HTML,URL)\n",
    "        features[\"HTML_FrequentDomain\"]=checkFrequentDomain(objects,HTML,URL)\n",
    "\n",
    "        features[\"HTML_Commonpage\"]=checkCommonPageRatioinWeb(objects,HTML,URL)\n",
    "        features[\"HTML_CommonPageRatioinFooter\"]=checkCommonPageRatioinFooter(HTML,URL)\n",
    "        features[\"HTML_SFH\"] = checkSFH(HTML, URL)\n",
    "        features[\"HTML_popUp\"] = checkPopUp(HTML)\n",
    "        features[\"HTML_RightClick\"] = checkRightClick(HTML)\n",
    "        features[\"HTML_DomainwithCopyright\"]=checkDomainwithCopyright(HTML,URL)\n",
    "        features[\"HTML_nullLinksinWeb\"]=nullLinksinWeb(HTML,URL)\n",
    "        features[\"HTML_nullLinksinFooter\"]=nullLinksinFooter(HTML, URL)\n",
    "        features[\"HTML_BrokenLink\"]=checkBrokenLink(HTML,URL)\n",
    "        features[\"HTML_LoginForm\"]=checkLoginForm(HTML,URL)\n",
    "        features[\"HTML_HiddenInfo_div\"]=checkHiddenInfo_div(HTML)  \n",
    "\n",
    "        features[\"HTML_HiddenInfo_button\"]=checkHiddenInfo_button(HTML)\n",
    "        features[\"HTML_HiddenInfo_input\"]=checkHiddenInfo_input(HTML)\n",
    "        features[\"HTML_TitleUrlBrand\"]=checkTitleUrlBrand(HTML,URL)\n",
    "        features[\"HTML_IFrame\"]=checkIFrame(HTML)\n",
    "        features[\"HTML_favicon\"] = checkFavicon(HTML, URL)\n",
    "        features[\"HTML_statusBarMod\"] = checkStatusBar(HTML)\n",
    "        features[\"HTML_css\"] = checkCSS(HTML,URL)\n",
    "        features[\"HTML_anchors\"] = checkAnchors(HTML, URL)                                        \n",
    "\n",
    "        #rep \n",
    "        images,links,anchors,audios,forms,css=get_object(HTML)                                              \n",
    "        features['external_item']=h_external(domain,hostname,images,links,anchors,audios,forms,css)\n",
    "        features['null_item']=h_null(images,links,anchors,audios,forms,css)\n",
    "        features['icp_code']=tian_check_icp(result_list,HTML)\n",
    "        features['e_cert']=e_certificate(HTML)\n",
    "        features['label']=info['label']\n",
    "        features['url']=URL\n",
    "        #print('url is',total_info['url'])\n",
    "        features['id']=pid\n",
    "        features['index']=int(i)\n",
    "#         print('features is',features)\n",
    "        with open(out_, 'a') as out:\n",
    "                    out.write('\\n')\n",
    "                    out.write(json.dumps(features))\n",
    "                    out.write(',')  \n",
    "        print('i is',i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d428ce29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "html success 091119\n",
      "i is 1030\n"
     ]
    }
   ],
   "source": [
    "chphish=read_json('../data/chiphish/ch_sites_total.json')\n",
    "extract(chphish,\"test_feature.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
